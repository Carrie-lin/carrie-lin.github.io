
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
    <title>Wenjia Wang</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://fonts.googleapis.com/css?family=Rubik:300,500,700" rel="stylesheet">
    <style>
        body{font-family:'Rubik',-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Oxygen,Ubuntu,Cantarell,'Open Sans','Helvetica Neue',sans-serif;font-size:calc(10px+0.33vw);-webkit-font-smoothing:antialiased;padding:5vh 20vw;color:#121314}p{font-weight:300}b{font-weight:bolder}a{color:#1367a7;text-decoration:none}a:hover:after{top:0}.table-img{width:30%;padding-top:20px;float:left}.table-content{width:70%;float:left;padding-bottom:20px}.paper-img{width:30%;float:left}@media screen and (max-device-width:700px){body{padding:5vh 5vw}.table-img{width:30%;padding-top:30px}.table-content{width:70%}}
    </style>
</head>

<body >
    <div width="100%">
        <div class="table-img">
            <img src="./assets/profile.jpg" width="50%" style="max-width: 200px;" alt="Wenjia Wang">
        </div>
        <div class="table-content" style="font-size:1em; line-height: 1">
            <p style="font-weight:500; font-size:1.8em">Wenjia Wang (王文佳)<br></p>
            <p>Ph.D Student </p>
            <p>The University of Hong Kong (HKU)</p>
            <p style="margin-top: 1em"><strong>Email</strong>: wwj2022@connect.hku.hk</p>
            <p><strong>More Links:</strong>: <a href="https://github.com/WenjiaWang0312" target="_blank">Github</a>,
                <a href="https://scholar.google.com/citations?user=cVWmlYQAAAAJ&hl=zh-CN&authuser=1" target="_blank">Scholar</a>,
                <a href="#main publications">Publications</a></p>
        </div>
    </div>

    <div style="font-size:1.2em; line-height: 1.4; margin-bottom: 50px;">

        <p>I'm currently a Ph.D. student <sup style="font-size:0.8em;">from 1.2023</sup> in the Department of Computer Science, 
            supervised by <a href="https://i.cs.hku.hk/~taku/" target="_blank">Prof. Taku Komura</a>. I am interested in Human Motion Capture and Synthesis.</p>
        
        <h1 id="Exprience">Experience</h1>
        <p>I got my B.Eng. from <b>NUAA</b> in 2016, majored in Aircraft Designing and M.Sc. degree from <b>Tongji University</b> in 2019, majored in Mechanical Engineering. 
            I worked in Sensetime from 2019 to 2020 as a research intern with <a href="https://scholar.google.com/citations?user=Dqjnn0gAAAAJ&hl=zh-CN" target="_blank
            ">Mr. Ding Liang</a>. From 2021 to 2022, I worked as an algorithm researcher in the Zoetrope Group (Used to be XLab) lead by <a href="https://scholar.google.com.hk/citations?user=jZH2IPYAAAAJ&hl=en" target="_
            _blank">Dr. Lei Yang</a>. I also worked as an algorithm researcher for a short time in Tencent in 2022. After that, I had an internship in <a href="https://www.shlab.org.cn/" target="_
            _blank">Shanghai AI Lab</a>.

    </div>
    
    <div>
        <h1 id="main publications">Main Publications</h1>


        <div style="font-size:1em; line-height: 1;">
            <div class="paper-img">
                <img src="./assets/publications/zolly.png" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh Reconstruction</b>
                <p><b>Wenjia Wang</b>, Yongtao Ge, Haiyi Mei, Zhongang Cai, Qingping Sun, Yanjun Wang, Chunhua Shen,
                    Lei Yang<sup>1</sup>, Taku Komura (1: corresponding author)</p>
                <p><b>arxiv 2023</b></p>
                <p>[<a href="https://github.com/WenjiaWang0312/Zolly" target="_blank">code&data</a>]
                    [<a
                    href="https://arxiv.org/abs/2303.13796" target="_blank">arxiv</a>]
                    </p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <p></p>

        <div style="font-size:1em; line-height: 1;">
            <div class="paper-img">
                <img src="./assets/publications/humman.png" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling</b>
                <p>Zhongang Cai*, Daxuan Ren*, Ailing Zeng*, Zhengyu Lin*, Tao Yu*, <b>Wenjia Wang*</b>,
                    Xiangyu Fan, Yang Gao, Yifan Yu, Liang Pan, Fangzhou Hong, Mingyuan Zhang, Chen Change Loy, 
                    Lei Yang<sup>1</sup>, Ziwei Liu<sup>1</sup> (*: equal contribution, 1: corresponding author)</p>
                <p><b>ECCV 2022 oral</b></p>
                <p>[<a href="https://caizhongang.github.io/projects/HuMMan/" target="_blank">webpage</a>][<a
                    href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670549.pdf" target="_blank">eccv pdf</a>]
                    [<a
                    href="https://arxiv.org/abs/2204.13686" target="_blank">arxiv</a>]
                    </p>
            </div>
            <div style="clear: both;"></div>
        </div>

        
    </div>


    
    <div>
        <h1 id="Projects">Projects</h1>

        <div style="font-size:1em; line-height: 1;">
            <div class="paper-img">
                <img src="./assets/mmhuman3d.gif" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">MMHuman3D</b>
                <p><a href="https://github.com/open-mmlab/mmhuman3d">MMHuman3D</a> is an open source
                PyTorch-based codebase for the use of 3D human parametric models. It is a part of the <a href="https://openmmlab.com/">OpenMMLab</a> project.
                <b>I am one of the main contributors, have contributed more than 20k lines of codes.</b>
                <p> - Reproducing popular methods with a modular framework</p>
                <p>- Supporting various datasets with a unified data convention</p>
                <p>- Versatile visualization toolbox</p>
    
                </p>
            </div>
            <div style="clear: both;"></div>
        </div>
    </div>

    <p></p>

    <div>
        <h1 id="previous publications">Previous Publications</h1>


        <div style="font-size:1em; line-height: 1;">
            <div class="paper-img">
                <img src="./assets/publications/trans10k.jpg" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">Segmenting transparent object in the wild with transformer</b>
                <p>Enze Xie, <b>Wenjia Wang</b>, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, Ping Luo</p>
                <p><b>IJCAI 2021</b></p>
                <p>[<a href="https://github.com/xieenze/Trans2Seg" target="_blank">code&data</a>][<a
                    href="https://www.ijcai.org/proceedings/2021/0165.pdf" target="_blank">ijcai pdf</a>]
                    [<a
                    href="https://arxiv.org/abs/2101.08461" target="_blank">arxiv</a>]
                    </p>
            </div>
            <div style="clear: both;"></div>
        </div>
        
        <p></p>


        <div style="font-size:1em; line-height: 1;">
            <div class="paper-img">
                <img src="./assets/publications/textzoom.png" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">Scene text image super-resolution in the wild</b>
                <p><b>Wenjia Wang</b>, Enze Xie, Xuebo Liu, Wenhai Wang, Ding Liang, Chunhua Shen, Xiang Bai</p>
                <p><b>ECCV 2020</b></p>
                <p>[<a href="https://github.com/WenjiaWang0312/TextZoom" target="_blank">code&data</a>][<a
                    href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550647.pdf" target="_blank">eccv pdf</a>]
                    [<a
                    href="https://arxiv.org/abs/2005.03341" target="_blank">arxiv</a>]
                    </p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <p></p>

        <div style="font-size:1em; line-height: 1;">
            <div class="paper-img">
                <img src="./assets/publications/translab.jpg" width="80%">
            </div>
            <div class="table-content">
                <b style="font-size:1.2em;">Segmenting Transparent Objects in the Wild</b>
                <p>Enze Xie, <b>Wenjia Wang</b>, Wenhai Wang, Mingyu Ding, Chunhua Shen, Ping Luo</p>
                <p><b>ECCV 2020</b></p>
                <p>[<a href="https://github.com/xieenze/Segment_Transparent_Objects" target="_blank">code&data</a>][<a
                    href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580681.pdf" target="_blank">eccv pdf</a>]
                    [<a
                    href="https://arxiv.org/abs/2003.13948" target="_blank">arxiv</a>]
                    </p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <p></p>

        
    </div>


    <div>
        <h1 id="Friends">Friends</h1>
        Some of my cooperators and friends: <a href="https://scholar.google.com.hk/citations?user=jZH2IPYAAAAJ&hl=en" target="__blank">Lei Yang (Sensetime, SH AI Lab)</a>,
        <a href="https://xieenze.github.io/">Enze Xie (Huawei Noah's Ark Lab)</a>, <a href="https://caizhongang.github.io/">Zhongang Cai (NTU, Sensetime)</a>,
        <a href="https://cshen.github.io/">Prof. Chunhua Shen (ZJU CAD Lab)</a>, <a href="https://frank-zy-dou.github.io/">Zhiyang Dou (HKU)</a>,
        <a href="https://scholar.google.co.uk/citations?user=GStTsxAAAAAJ&hl=zh-CN">Jingbo Wang (CUHK, SH AI Lab)</a>.
        
    </div>

</body>

</html>